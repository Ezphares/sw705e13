\section{The GX and UX-Method}
\label{sec:gx_ux_method}
UX is a development method which bases development of a given software product on User Experience (UX).
While we did not follow the UX development method during this project, a highly interesting paper on user evaluation of a game, was based on this development method.
The paper saw shortcomings in the way UX was used to test videogames, which inspired the writers of this paper to produce their own method, named Gameplay Experience (GX).
We determined that though it would not be possible for us to carry out exactly the test described in the paper introducing GX, the way GX thinks about user evaluation of games could be directly translated to this project.

\todo{Doublecheck + insert source send by Martin}
\subsection{The Three Boxes of GX}
GX splits testing of a game into three boxes or layers.
Each box has its own area of responsibility and goals, which allows testers better clarity of what part of the game is good, and which parts are not.
Furthermore GX offers inspiration and ideas on how to measure the findings of a user evaluation.
Doing a three-way split of the evaluation also ensures that developers are better off in understanding where the shortcomings of the games are, for as GX shows us, it is not always enough to simply evaluate a game - or a piece of software for that matter - against a set demography.

\subsection{1st Box - Game System Experience}
The Game System Experience primarily focuses on testing the game in a very specific, software oriented way.
This testing phase does not consider any sort of testing wherein a user is involved, rather it strictly focuses on ensuring that the game runs as expected.
This is typically done via, for example: Unit Testing, Code Coverage, Stress Testing, Gameplay Metrics and Hardware Tests - in the paper there are more examples on how a Game System evaluation can be conducted.
It is not necessary to go through with every single kind of test described in the paper however, but it goes without saying, that the more thoroughly tested the software is, the greater are the chances that the program will run as expected.

\subsection{2nd Box - Individual Player Experience}
The Individual Player Experience assessment is where tests on an actual player of the game are conducted.
One of the main challenges with assessing Player Experience is, how does one properly evaluate a players' emotions during playing a game?
Such as, are they frustrated by certain game aspects, are there parts of the game, which the developer wants to be fun and engaging, but winds up boring to the player?
These things can be difficult to get an unambiguous answer to - but more importantly, for serious games such as ours whose purpose is to teach - how do we make sure that the player actually learns from playing our game?
These questions belongs to the \textbf{Psycophysiological} aspect of Player Testing.
The solution is to make use of modern sensors and new technology such as brain scanners, to see which parts of the brain are activated during certain parts of the game.
For example, assuming the game to test aspires to teach, it would be possible to monitor the sections of the brain that are most used during learning.
If these are highly active, chances are the game manages to teach in some form.
There are other ways of testing this as well, such as sensors placed on sweat glands, to test levels of 'brain intensity' and stress.\\

If it is not possible to make an elaborate Psycophysiological test of the player, there are other options available, such as engaging the player in interviews, questionnaires, input logging (software based - check a player's input and analyze, and / or ask questions in regards to their input), player modelling (create an AI that simulates a player) and eye tracking.


\subsection{3rd Box - Player Context Experience}
The last layer of assessment is often overlooked by game development teams according to \todo{Insert: source} but is no less important.
This kind of evaluation corresponds to etnographical or location-based testing.
This is typically done on games for a mobile platform, where the context in which the game is played has a huge 'spread' and could be anywhere from being on the move to at home in the couch, but it is also about how the game is perceived in a multiplayer context - such as what is the social experience, from interacting with other players in the game.
Aside from these metrics, it is also possible to do Playability Heuristics which covers the possibility of having an expert give their review of the game, according to their expert knowledge, this is often considered the go-to choice by most game developers, because it is cheap and time efficient.
Cultural debugging is also part of this layer, to ensure that there are no cultural mishaps between what message the developers wanted to convey, and what the player actually understands because of the cultural differences.

\section{Our Test-method}
\label{sec:test_method}

\subsection{Limitations}

When we take into account the GX method, it is apparent to us, that there are things we cannot test for - it is simply out of scope, for a university 
project of this magnitude. For example, testing of player context, it would only be viable for us to test, how well the game plays while on-the-move, 
in terms of whether the lights and colors on the screen are clear enough, however hiring an expert in the field, to aid with cultural debugging, is out 
of scope when the target group for this project does not imply cultural differences. With this in mind, it is clear that to test the player context, 
the application needs to be mobile, which it is not at this stage.\todo{Doublecheck: when game is done}


In terms of evaluating the player, it is not possible for us, to set up a psycophysiological test, as that would demand access to hardware that we do 
not currently posses. Finally, in terms of testing the system, given most of these tests are all software based, we are able to do all of them, so it is simply a matter of chosing which ones, we feel are the most relevant.

\subsection{The Set-up}

The test will be conducted by requesting a server from the University to host our game, which will then be accessible to the outside world, by simply visiting a website. Opening up the game to the entire world, ensures that we have a high chance of getting many people, to test our game which gives the test much more of a quantitative advantage, than necesserily a qualitative advantage. This will be taken into consideration, when we decide how to test the system, and make the player evaluation.

\subsection{Testing the System}

We have decided to employ \textbf{Unit Tests} to both check that the code works as intended, but the more code we are able to unit test, the more do we ensure a high code coverage on our tests, so it feels like a once-the-effort-twice-the-pay situation for us. Furthermore, we will use player logging on our system, to record what buttons the user presses, to get an idea of how clear the software is to use during a playthrough, whether they chose the direct-path through our game, or they have trouble figuring out, what to do.\todo{Doublecheck: when gone is done.}

\subsection{Testing the Player}

Given the aforementioned limitations, the only viable test mechanisms we can use for this sort of quantitative evaluation, are questionnaires and interviews (/feedback) which we harvest, from the users when they test our game. This decision is further backed up, by the way we wish to conduct the test. If we are able to have every person who tests the software fill out a questionnaire and give us feedback, there will be sufficient data to go through.

\subsection{Questionnaire}

\subsection{Results}