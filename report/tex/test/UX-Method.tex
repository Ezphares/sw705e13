\section{The GX and UX-Method}

UX is a development method which bases development of a given software product on User Experience (UX). While we did not follow the UX development 
method during this project, a highly interesting paper on user evaluation of a game, was based on this development method. The paper saw shortcomings 
in the way UX was used to test videogames, which inspired the writers of this paper to produce their own method, named Gameplay Experience (GX).
We determined that though it would not be possible for us to carry out exactly the test described in the paper introducing GX, the way GX thinks about 
user evaluation of games could be directly translated to this project.

\todo{doublecheck og inds√¶t kilden Martin sendte}
\subsection{The Three Boxes of GX}

GX splits testing of a game into three boxes or layers. Each box has its own area of responsibility and goals, which allows testers better clarity of 
what part of the game is good, and which parts are not. Furthermore GX offers inspiration and ideas on how to measure the findings of a user 
evaluation. Doing a three-way split of the evaluation also ensures that developers are better off in understanding where the shortcomings of the games 
are, for as GX shows us, it is not always enough to simply evaluate a game - or a piece of software for that matter - against a set demography.

\subsection{1st Box - Game System Experience}

The Game System Experience primarily focuses on testing the game in a very specific, software oriented way. This testing phase does not consider any 
sort of testing wherein a user is involved, rather it strictly focuses on ensuring that the game runs as expected. This is typically done via, for 
example: Unit Testing, Code Coverage, Stress Testing, Gameplay Metrics and Hardware Tests - in the paper there are more examples on how a Game System 
evaluation can be conducted. It is not necessary to go through with every single kind of test described in the paper however, but it goes without 
saying, that the more thoroughly tested the software is, the greater are the chances that the program will run as expected.

\subsection{2nd Box - Individual Player Experience}

The Individual Player Experience assessment is where tests on an actual player of the game are conducted. One of the main challenges with assessing 
Player Experience is, how does one properly evaluate a players' emotions during playing a game? Such as, are they frustrated by certain game aspects, 
are there parts of the game, which the developer wants to be fun and engaging, but winds up boring to the player? These things can be difficult to get 
an unambiguous answer to - but more importantly, for serious games such as ours whose purpose is to teach - how do we make sure that the player 
actually learns from playing our game? These questions belongs to the \textbf{Psycophysiological} aspect of Player Testing. The solution is to make use 
of modern sensors and new technology such as brain scanners, to see which parts of the brain are activated during certain parts of the game. For 
example, assuming the game to test aspires to teach, it would be possible to monitor the sections of the brain that are most used during learning. If 
these are highly active, chances are the game manages to teach in some form. There are other ways of testing this as well, such as sensors placed on 
sweat glands, to test levels of 'brain intensity' and stress.


If it is not possible to make an elaborate Psycophysiological test of the player, there are other options available, such as engaging the player in 
interviews, questionnaires, input logging (software based - check a player's input and analyze, and / or ask questions in regards to their input), 
player modelling (create an AI that simulates a player) and eye tracking.


\subsection{3rd Box - Player Context Experience}

The last layer of assessment is often overlooked by game development teams according to \todo{indsaet kilde} but is no less important. This kind of 
evaluation corresponds to etnographical or location-based testing. This is typically done on games for a mobile platform, where the context in which 
the game is played has a huge 'spread' and could be anywhere from being on the move to at home in the couch, but it is also about how the game is 
perceived in a multiplayer context - such as what is the social experience, from interacting with other players in the game. Aside from these metrics, 
it is also possible to do Playability Heuristics which covers the possibility of having an expert give their review of the game, according to their 
expert knowledge, this is often considered the go-to choice by most game developers, because it is cheap and time efficient. Cultural debugging is also 
part of this layer, to ensure that there are no cultural mishaps between what message the developers wanted to convey, and what the player actually 
understands because of the cultural differences.

\section{Our Test-method}

\todo{Nedenstaaende udfyldes som programmet bliver faerdigt og vi finder ud af, hvad vi kan}
\subsection{Questionnaire}

\subsection{Results}